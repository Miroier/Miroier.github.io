<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#6667ab"><meta name="generator" content="Hexo 7.2.0">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#6667ab">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"yoursite.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"flat"},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false,"trigger":"auto"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本文将首先从公式出发，讲解 3-pass safe softmax、 2-pass online softmax 以及 1-pass FlashAttention 的原理。然后结合论文介绍 FlashAttention 1&#x2F;2&#x2F;3。">
<meta property="og:type" content="article">
<meta property="og:title" content="FlashAttention：从 1 到 3">
<meta property="og:url" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/index.html">
<meta property="og:site_name" content="Seclusion">
<meta property="og:description" content="本文将首先从公式出发，讲解 3-pass safe softmax、 2-pass online softmax 以及 1-pass FlashAttention 的原理。然后结合论文介绍 FlashAttention 1&#x2F;2&#x2F;3。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-1.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-2.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-3.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-4.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-5.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-6.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-7.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-8.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-9.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-10.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-11.png">
<meta property="og:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-12.png">
<meta property="article:published_time" content="2024-05-30T13:11:49.000Z">
<meta property="article:modified_time" content="2024-05-30T13:11:49.000Z">
<meta property="article:author" content="Miroier">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image.png">


<link rel="canonical" href="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/","path":"2024/05/30/FlashAttention：从-1-到-3/","title":"FlashAttention：从 1 到 3"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>FlashAttention：从 1 到 3 | Seclusion</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Seclusion</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">2</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">1</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">19</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8E-saft-softmax-%E5%88%B0-flashattention1"><span class="nav-text">从 saft softmax 到
FlashAttention1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#flashattention2"><span class="nav-text">FlashAttention2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#flashattention3"><span class="nav-text">FlashAttention3</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Miroier"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Miroier</p>
  <div class="site-description" itemprop="description">keep calm and carry on</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Miroier" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Miroier" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://www.ryuuyou.cn/" title="https:&#x2F;&#x2F;www.ryuuyou.cn&#x2F;" rel="noopener" target="_blank">RyuuYou</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://www.gislxz.top/" title="http:&#x2F;&#x2F;www.gislxz.top&#x2F;" rel="noopener" target="_blank">giser吱吱</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://dimsmary.tech/" title="https:&#x2F;&#x2F;dimsmary.tech&#x2F;" rel="noopener" target="_blank">Dimsmary</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://sydddl.github.io/" title="https:&#x2F;&#x2F;sydddl.github.io&#x2F;" rel="noopener" target="_blank">DeathSprout</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://syvshc.github.io/" title="https:&#x2F;&#x2F;syvshc.github.io&#x2F;" rel="noopener" target="_blank">无锤乙醇</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://myqaq.cn/" title="https:&#x2F;&#x2F;myqaq.cn&#x2F;" rel="noopener" target="_blank">双语之城</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://peng-yq.github.io/" title="https:&#x2F;&#x2F;peng-yq.github.io&#x2F;" rel="noopener" target="_blank">PYQ</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://baozi.run/" title="https:&#x2F;&#x2F;baozi.run&#x2F;" rel="noopener" target="_blank">江盟</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://blog.bj-yan.top/" title="https:&#x2F;&#x2F;blog.bj-yan.top&#x2F;" rel="noopener" target="_blank">北屿</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://windy810.github.io/" title="https:&#x2F;&#x2F;windy810.github.io&#x2F;" rel="noopener" target="_blank">Windy</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://qgrain.github.io/" title="https:&#x2F;&#x2F;qgrain.github.io&#x2F;" rel="noopener" target="_blank">Zhiyu</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://zimingyuan.github.io/friends.html" title="https:&#x2F;&#x2F;zimingyuan.github.io&#x2F;friends.html" rel="noopener" target="_blank">VnYzm</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://zhangzhao219.github.io/" title="https:&#x2F;&#x2F;zhangzhao219.github.io&#x2F;" rel="noopener" target="_blank">zhangzhao219</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://catigeart.github.io/" title="https:&#x2F;&#x2F;catigeart.github.io&#x2F;" rel="noopener" target="_blank">Catigeart</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://gladingray.github.io/" title="https:&#x2F;&#x2F;gladingray.github.io" rel="noopener" target="_blank">GladingRay</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Miroier">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Seclusion">
      <meta itemprop="description" content="keep calm and carry on">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="FlashAttention：从 1 到 3 | Seclusion">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          FlashAttention：从 1 到 3
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-30 21:11:49" itemprop="dateCreated datePublished" datetime="2024-05-30T21:11:49+08:00">2024-05-30</time>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>本文将首先从公式出发，讲解 3-pass safe softmax、 2-pass online
softmax 以及 1-pass FlashAttention 的原理。然后结合论文介绍
FlashAttention 1/2/3。</p>
<span id="more"></span>
<h1 id="从-saft-softmax-到-flashattention1">从 saft softmax 到
FlashAttention1</h1>
<p>Self-Attention 的计算公式如下（其中，<span
class="math inline">\(O,Q,K,V\)</span> 都是 <span
class="math inline">\((N,d)\)</span> 形状的矩阵，<span
class="math inline">\(N\)</span> 是 sequence length，<span
class="math inline">\(d\)</span> 是 head
dimension，为了简单起见，忽略了缩放因子 <span
class="math inline">\(\frac{1}{\sqrt {d}}\)</span>）：</p>
<p><span class="math display">\[
O=\mathrm{softmax} (QK^T)V \tag{1}
\]</span></p>
<p>标准的计算流程分为几步：</p>
<p><span class="math display">\[
\begin{align}
X &amp; =Q K^{T} \tag{2}\\
A &amp; =\operatorname{softmax}(X) \tag{3}\\
O &amp; =A V \tag{4}
\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(X\)</span> 是 softmax
前的临时值，<span class="math inline">\(A\)</span> 是注意力分数，<span
class="math inline">\(O\)</span> 是输出。</p>
<p>在 FlashAttention 中，不需要在全局内存中生成 X 和 A，而是将公式 1
的计算融合到一个单一的 cuda kernel 中。</p>
<p>在像矩阵乘法这样的经典算法中，可以使用分块（tiling）的方法，确保片上内存可以容纳下参与计算的数据。分块的前提是，运算满足结合律，即整个矩阵乘法的结果可以分解为许多分块矩阵乘法的和。</p>
<p>但是在 Self-Attention 中，softmax
看上去并不满足结合律，因此无法像矩阵乘法那样分块运算，有什么方法可以让
softmax 也满足结合律吗？</p>
<p>首先来回顾一下 softmax 是如何计算的，下面是标准 softmax
的计算公式：</p>
<p><span class="math display">\[
\mathrm{soft}\max \left( \left\{ x_1,x_2,\dots ,x_N \right\} \right)
=\left\{ \frac{e^{x_i}}{\sum\nolimits_{j=1}^N{e^{x_j}}} \right\}
_{i=1}^{N} \tag{5}
\]</span></p>
<p>由于 <span class="math inline">\(e^x\)</span>
非常容易溢出，因此实际应用中，一般会通过一个等价变换变成 safe
softmax：</p>
<p><span class="math display">\[
\frac{e^{x_i}}{\sum\nolimits_{j=1}^N{e^{x_j}}}=\frac{e^{x_i-m}}{\sum\nolimits_{j=1}^N{e^{x_j-m}}}
\tag{6}
\]</span></p>
<p>其中 <span class="math inline">\(m=\max _{j=1}^{N}\left( x_j
\right)\)</span>。</p>
<p>对 safe softmax 的计算包括三次遍历，一般也叫 <strong>3-pass safe
softmax</strong>。</p>
<p>设 <span class="math inline">\(m_i\)</span> 表示 <span
class="math inline">\(\max _{j=1}^{i}\left( x_j \right)\)</span>，<span
class="math inline">\(m_0=-\infty\)</span>；<span
class="math inline">\(l_i\)</span> 表示 <span
class="math inline">\(\sum\nolimits_{j=1}^N{e^{x_j-m_N}}\)</span>，<span
class="math inline">\(l_0=0\)</span>，<span
class="math inline">\(a_i\)</span> 表示最终结果</p>
<p>第一次 pass 得到最大值 <span class="math inline">\(m_N\)</span>：</p>
<p><span class="math display">\[
m_i=\max \left( m_{i-1},x_i \right) \tag{7}
\]</span></p>
<p>第二次 pass 得到分母项 <span class="math inline">\(l_N\)</span>：</p>
<p><span class="math display">\[
l_i=l_{i-1}+e^{x_i-m_N} \tag{8}
\]</span></p>
<p>第三次 pass 得到 <span class="math inline">\(a_i\)</span>：</p>
<p><span class="math display">\[
a_i=\frac{e^{x_i-m_N}}{l_N} \tag{9}
\]</span></p>
<p>看上去公式 8 依赖于 <span class="math inline">\(m_N\)</span>，公式 9
依赖于 <span class="math inline">\(m_N\)</span> 和 <span
class="math inline">\(l_N\)</span>，似乎没有办法减少 pass
的次数？其实是可以的，这个技巧是用 <span
class="math inline">\(l_{i}^{\prime}=\sum\nolimits_{j=1}^i{e^{x_j-m_i}}\)</span>
替换 <span class="math inline">\(l_i\)</span>，它们的最终结果 <span
class="math inline">\(l_{N}^{\prime}\)</span> 和 <span
class="math inline">\(l_N\)</span> 是相同的，因此公式 9
可以简单替换一下写成 <span
class="math inline">\(a_i=\frac{e^{x_i-m_N}}{l_{N}^{\prime}}\)</span>。</p>
<p><span class="math inline">\(l_{i}^{\prime}\)</span>
的更新方法是：</p>
<p><span class="math display">\[
\begin{align}
    l_{i}^{\prime}&amp;=\sum_{j=1}^i{e^{x_j-m_i}}\\
    &amp;=\left( \sum_{j=1}^{i-1}{e^{x_j-m_i}} \right) +e^{x_i-m_i}\\
    &amp;=\left( \sum_{j=1}^{i-1}{e^{x_j-m_{i-1}}} \right)
e^{m_{i-1}-m_i}+e^{x_i-m_i}\\
    &amp;=l_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\
\end{align} \tag{10}
\]</span></p>
<p>可以看出 <span class="math inline">\(l_{i}^{\prime}\)</span>
的更新就只依赖于 <span class="math inline">\(m_{i-1}\)</span> 和 <span
class="math inline">\(m_i\)</span> 而不是依赖于 <span
class="math inline">\(m_N\)</span>。</p>
<p>这样就可以将公式 7 和公式 8 的两次 pass 合并为一次
pass，这种算法也被称为 <strong>2-pass online softmax</strong>。</p>
<p>第一次 pass 得到 <span class="math inline">\(m_N\)</span> 和 <span
class="math inline">\(l_N\)</span>：</p>
<p><span class="math display">\[
\begin{align}
    m_i&amp;=\max \left( m_{i-1},x_i \right)\\
    l_{i}^{\prime}&amp;=l_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\
\end{align}
\]</span></p>
<p>第二次 pass 得到 <span class="math inline">\(a_i\)</span>：</p>
<p><span class="math display">\[
a_i=\frac{e^{x_i-m_N}}{l_{N}^{\prime}}
\]</span></p>
<p>第一次 pass 中，用 <span class="math inline">\(m_i\)</span> 对 <span
class="math inline">\(l_{i-1}^{\prime}\)</span> 更新的行为在论文中叫做
rescale，FA2 论文中还会提到这一点。</p>
<p>但是这依然需要两次 pass，是否有方法能够实现一次 pass 的
softmax？这实际上是不可能的，但是在 Self-Attention
中，我们的目标并不是注意力分数 <span
class="math inline">\(A\)</span>，而是输出 <span
class="math inline">\(O\)</span>，而 <span
class="math inline">\(O\)</span> 的一次 pass 是存在的。</p>
<p>下面首先介绍一下 <strong>Multi-pass
Self-Attention</strong>。由于所有行的计算是独立的，为了简单只展示第 k
行的计算。</p>
<p>设 <span class="math inline">\(Q[k,:]\)</span> 表示 <span
class="math inline">\(Q\)</span> 的第 k 行，<span
class="math inline">\(K^T[:,i]\)</span> 表示 <span
class="math inline">\(K^T\)</span> 的第 i 列，<span
class="math inline">\(O[k,:]\)</span> 表示 <span
class="math inline">\(O\)</span> 的第 k 行，<span
class="math inline">\(V[i,:]\)</span> 表示 <span
class="math inline">\(V\)</span> 的第 i 行，<span
class="math inline">\(o_i=\sum\nolimits_{j=1}^i{a_jV\left[ j,:
\right]}\)</span> 是一个用来存 <span class="math inline">\(A[k,:i]\times
V[:,:i]\)</span> 部分结果的行向量。</p>
<p>第一次 pass，得到 <span class="math inline">\(m_N\)</span> 和 <span
class="math inline">\(l_N\)</span>：</p>
<p><span class="math display">\[
\begin{align}
    x_i&amp;=Q[k,:]K^T[:,i]\\
    m_i&amp;=\max \left( m_{i-1},x_i \right)\\
    l_{i}^{\prime}&amp;=l_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\
\end{align}
\]</span></p>
<p>第二次 pass，得到 <span
class="math inline">\(\boldsymbol{o}_N\)</span>：</p>
<p><span class="math display">\[
\begin{align}
    a_i&amp;=\frac{e^{x_i-m_N}}{l_{N}^{\prime}} \tag{11}\\
    \boldsymbol{o}_i&amp;=\boldsymbol{o}_{i-1}+a_iV[i,:] \tag{12}\\
\end{align}
\]</span></p>
<p>最后将 <span class="math inline">\(\boldsymbol{o}_N\)</span> 写入到
<span class="math inline">\(O\)</span>：</p>
<p><span class="math display">\[
O[k,:]=\boldsymbol{o}_N
\]</span></p>
<p>将公式 11 代入进公式 12，可以得到：</p>
<p><span class="math display">\[
\boldsymbol{o}_i=\sum_{j=1}^i{\frac{e^{x_j-m_N}}{l_{N}^{\prime}}V[j,:]}
\tag{13}
\]</span></p>
<p>下面是 <strong>Multi-pass Self-Attention</strong> 的示意图。</p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image.png" class="" title="Alt text">
<p><span class="math inline">\(\boldsymbol{o}_N\)</span> 的结果依赖于
<span class="math inline">\(m_N\)</span> 和 <span
class="math inline">\(l_{N}^{\prime}\)</span>，这和之前在 <strong>3-pass
safe softmax</strong> 中遇到的问题很相似，因此解决技巧也是相似的，使用
<span class="math inline">\(\boldsymbol{o}^{\prime}\)</span> 替换 <span
class="math inline">\(\boldsymbol{o}\)</span>。</p>
<p><span class="math display">\[
\boldsymbol{o}_{i}^{\prime}=\left(
\sum_{j=1}^i{\frac{e^{x_j-m_i}}{l_{i}^{\prime}}V[j,:]} \right)
\]</span></p>
<p><span class="math inline">\(\boldsymbol{o}_N^{\prime}\)</span> 和
<span class="math inline">\(\boldsymbol{o}_N\)</span> 显然相同，<span
class="math inline">\(\boldsymbol{o}_i^{\prime}\)</span>
的更新公式是：</p>
<p><span class="math display">\[
\begin{align}
    \boldsymbol{o}_{i}^{\prime}&amp;=\left(
\sum_{j=1}^i{\frac{e^{x_j-m_i}}{l_{i}^{\prime}}V[j,:]} \right)\\
    &amp;=\left(
\sum_{j=1}^{i-1}{\frac{e^{x_j-m_i}}{l_{i}^{\prime}}V[j,:]} \right)
+\frac{e^{x_i-m_i}}{l_{i}^{\prime}}V[i,:]\\
    &amp;=\left(
\sum_{j=1}^{i-1}{\frac{e^{x_j-m_{i-1}}}{l_{i-1}^{\prime}}\frac{e^{x_j-m_i}}{e^{x_j-m_{i-1}}}\frac{l_{i-1}^{\prime}}{l_{i}^{\prime}}V[j,:]}
\right) +\frac{e^{x_i-m_i}}{l_{i}^{\prime}}V[i,:]\\
    &amp;=\left(
\sum_{j=1}^{i-1}{\frac{e^{x_j-m_{i-1}}}{l_{i-1}^{\prime}}V[j,:]} \right)
\frac{l_{i-1}^{\prime}}{l_{i}^{\prime}}e^{m_{i-1}-m_i}+\frac{e^{x_i-m_i}}{l_{i}^{\prime}}V[i,:]\\
    &amp;=\boldsymbol{o}_{i-1}^{\prime}
\frac{l_{i-1}^{\prime}e^{m_{i-1}-m_i}}{l_{i}^{\prime}}+\frac{e^{x_i-m_i}}{l_{i}^{\prime}}V[i,:]\\
\end{align} \tag{14}
\]</span></p>
<p><span class="math inline">\(\boldsymbol{o}_i^{\prime}\)</span> 依赖于
<span
class="math inline">\(m_i,m_{i-1},l_{i}^{\prime},l_{i-1}^{\prime}\)</span>
而不是 <span
class="math inline">\(m_N,l_{N}^{\prime}\)</span>。这样所有的计算只需要一次
pass 即可完成，这就是 <strong>FlashAttention</strong>：</p>
<p><span class="math display">\[
\begin{align}
    x_i&amp;=Q[k,:]K^T[:,i]\\
    m_i&amp;=\max \left( m_{i-1},x_i \right)\\
    l_{i}^{\prime}&amp;=l_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\
    \boldsymbol{o}_{i}^{\prime}&amp;=\boldsymbol{o}_{i-1}^{\prime}\frac{l_{i-1}^{\prime}e^{m_{i-1}-m_i}}{l_{i}^{\prime}}+\frac{e^{x_i-m_i}}{l_{i}^{\prime}}V[i,:]\\
\end{align} \tag{15}
\]</span></p>
<p><span class="math display">\[
O[k,:]=\boldsymbol{o}_N^{\prime}
\]</span></p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-1.png" class="" title="Alt text">
<p><span
class="math inline">\(x_i,m_i,l_{i}^{\prime},\boldsymbol{o}_i^{\prime}\)</span>
的存储占用较小，可以放入 GPU
的共享内存中。并且由于该算法中的所有操作都是满足结合律的，因此与分块（tiling）兼容。</p>
<p><strong>FlashAttention(Tiling)</strong></p>
<p>将 <span class="math inline">\(K^T\)</span> 分成多个块，假设 <span
class="math inline">\(b\)</span> 表示 tile 的 block size，<span
class="math inline">\(tiles\)</span> 表示行方向有多少个 tile，<span
class="math inline">\(N=b\times tiles\)</span>，<span
class="math inline">\(\boldsymbol{x}_i\)</span> 表示存 <span
class="math inline">\(Q[k]K^T\)</span> 结果的第 i 个 tile <span
class="math inline">\([(i-1)b:ib]\)</span> 的向量，<span
class="math inline">\(m_i^{local}\)</span> 表示 <span
class="math inline">\(x_i\)</span> 中的局部最大值。</p>
<p><span class="math inline">\(i\gets 1,tiles\)</span></p>
<p><span class="math display">\[
\begin{align}
    \boldsymbol{x}_i&amp;=Q[k,:]K^T[:,\left( i-1 \right) b:ib]\\
    m_{i}^{local}&amp;=\underset{j=1}{\overset{b}{\max}}\left(
\boldsymbol{x}_i \right)\\
    m_i&amp;=\max \left( m_{i-1},m_{i}^{local} \right)\\
    l_{i}^{\prime}&amp;=l_{i-1}^{\prime}e^{m_{i-1}-m_i}+\sum_{j=1}^b{e^{\boldsymbol{x}_i\left[
j \right] -m_i}}\\
    \boldsymbol{o}_{i}^{\prime}&amp;=\boldsymbol{o}_{i-1}^{\prime}\frac{d_{i-1}^{\prime}e^{m_{i-1}-m_i}}{l_{i}^{\prime}}+\sum_{j=1}^b{\frac{e^{\boldsymbol{x}_i\left[
j \right] -m_i}}{l_{i}^{\prime}}V[j+\left( i-1 \right) b,:]}\\
\end{align} \tag{16}
\]</span></p>
<p><span class="math display">\[
O[k,:]=\boldsymbol{o}_{N/b}^{\prime}
\]</span></p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-2.png" class="" title="Alt text">
<p>上图说明了 FlashAttention 在硬件上的计算方式。蓝色的块表示驻留在 SRAM
中的 tile，而红色的块对应于第 <span class="math inline">\(k\)</span>
行。L 表示 sequence length，可以相当大（例如16k），d 通常在 Transformers
中较小（例如GPT3的128），b 是可以控制的块大小。值得注意的是，整体 SRAM
内存占用仅依赖于 b 和 d，而与 N 无关。因此，该算法能够扩展到较长的
sequence length 而不会遇到内存问题。在计算过程中，从左到右遍历 <span
class="math inline">\(K^T\)</span> 和 <span
class="math inline">\(A\)</span> ，从上到下遍历 <span
class="math inline">\(V\)</span>，并相应地更新 <span
class="math inline">\(m,l,O\)</span> 的状态。</p>
<p>以上证明来自于 《From Online Softmax to
FlashAttention》。理解了上述过程，FlashAttention1
论文中的相应证明也就好理解了。<a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642962397">万字长文详解FlashAttention
v1/v2 - 知乎</a> 这篇文章中详细介绍了论文中的证明。</p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-3.png" class="" title="Alt text">
<p>上图是 FlashAttention1 论文中的算法伪代码。对 <span
class="math inline">\(Q,O\)</span> 沿着行方向分成 <span
class="math inline">\(T_r\)</span> 块，每一块的大小是 <span
class="math inline">\(B_r\times d\)</span>，对 <span
class="math inline">\(K,V\)</span> 沿着行方向分为 <span
class="math inline">\(T_c\)</span> 块，每一块的大小为 <span
class="math inline">\(B_c\times d\)</span>。</p>
<p>伪代码中带波浪线的是局部值，<span class="math inline">\(\tilde{m}_{i
j}\)</span> 即为下图中的 <span class="math inline">\(m_{i
j}\)</span>，其它类似。第 11 行更新 <span
class="math inline">\(l^{new}_i\)</span> 看上去和公式 16
中的不太对应，这是因为在第 10 行，用 <span
class="math inline">\(\tilde{m}_{i j}\)</span> 这个局部值计算得到 <span
class="math inline">\(\tilde{P}_{i j}\)</span>，再得到 <span
class="math inline">\(\tilde{l}_{i j}\)</span>，所以第 11 要用 <span
class="math inline">\(m^{new}_i\)</span> 去更新 <span
class="math inline">\(\tilde{l}_{i j}\)</span>。</p>
<p>下图是伪代码的示意图，虚线上的数字对应伪代码中的行数。[[极简
FlashAttention CUDA 实现]]
这个仓库的代码和为代码能对应上，结合伪代码、代码、示意图，应该就能看懂
FlashAttention1 的过程了。</p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-4.png" class="" title="Alt text">
<p>这里分块参数 <span class="math inline">\(B_c,B_r\)</span> 的选取和
SRAM 的大小 M 有关，<span class="math inline">\(B_c=\lceil \frac{M}{4d}
\rceil ,B_r=\min \left( \lceil \frac{M}{4d} \rceil ,d
\right)\)</span></p>
<p>这样选择的原因是为了尽量用满 SRAM：</p>
<p><span class="math display">\[
\begin{align}
    SRAM\left( Q_i \right) &amp;=B_r\times d=\min \left( \lceil
\frac{M}{4d} \rceil ,d \right) \times d&lt;\left. \lceil \frac{M}{4}
\rceil \right.\\
    SRAM\left( O_i \right) &amp;=B_r\times d=\min \left( \lceil
\frac{M}{4d} \rceil ,d \right) \times d&lt;\lceil \frac{M}{4} \rceil\\
    SRAM\left( K_j,V_j \right) &amp;=2\times B_c\times d=2\times \lceil
\frac{M}{4d} \rceil \times d&lt;\lceil \frac{M}{2} \rceil\\
\end{align}
\]</span></p>
<p>FlashAttention
在前向计算时不保留中间结果，但是反向计算又需要这些中间结果，FlashAttention
backward pass 的处理方式是重计算，和 forward 一样，将 Q、K、V 分块读取到
SRAM 中，并计算得到当前块的中间结果。对比 Self-Attention 和
FlashAttention 的伪代码：</p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-5.png" class="" title="Alt text">
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-6.png" class="" title="Alt text">
<p>可以发现，在 FlashAttention backward pass 中，减少了对 <span
class="math inline">\(S,dS,P,dP\)</span>
这些中间结果的全局内存读写。虽然这样增加了计算量的开销，但是减少 IO
开销的收益更大。</p>
<p>IO 分析：</p>
<p>对于 Self-Attention 从算法 3 可知，第一行读 <span
class="math inline">\(Q,K\)</span> 是 <span
class="math inline">\(2Nd\)</span>，写 S 是 <span
class="math inline">\(N^2\)</span>，第二行读 <span
class="math inline">\(S\)</span> 是 <span
class="math inline">\(N^2\)</span>，写 <span
class="math inline">\(P\)</span> 是 <span
class="math inline">\(N^2\)</span>，第三行读 <span
class="math inline">\(P\)</span> 是 <span
class="math inline">\(N^2\)</span>，读 <span
class="math inline">\(V\)</span> 是 <span
class="math inline">\(Nd\)</span>，写 <span
class="math inline">\(O\)</span> 是 <span
class="math inline">\(Nd\)</span>。忽略常数，IO 复杂度是 <span
class="math inline">\(O(Nd+N^2)\)</span>。</p>
<p>对于 FA1，内循环读 <span class="math inline">\(Q\)</span> 是 <span
class="math inline">\(Nd\)</span>，外循环决定了内循环的次数，即 <span
class="math inline">\(T_c=\lceil \frac{4dN}{M} \rceil\)</span>
次，忽略常数，IO 复杂度是 <span
class="math inline">\(O(N^2d^2M^{-1})\)</span>。</p>
<p>由于 M 远大于 d，所以 FA1 IO 复杂度远小于 Self-Attention。</p>
<h1 id="flashattention2">FlashAttention2</h1>
<p>FlashAttention2 的核心思路与 1
相同，但是做了一些工程上的优化。这包括：</p>
<p><strong>减少了非矩阵乘法运算，增加 Tensor Core
使用比例。</strong></p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-7.png" class="" title="Alt text">
<p>在 Tensor Core
的帮助下，理论上时间复杂度更高的矩阵乘可能实际运行起来会那些复杂度低的操作更快更快。比如这个把
FFT 改成矩阵乘的工作 <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.05908">[2311.05908] FlashFFTConv:
Efficient Convolutions for Long Sequences with Tensor Cores</a>。</p>
<p>在 FlashAttention 1 的 forward pass 中：</p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-8.png" class="" title="Alt text">
<p>而在 FlashAttention 2 中：</p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-9.png" class="" title="Alt text">
<p>区别是，在 2 中，不是迭代计算都执行 rescale 操作，而是在最后执行一次
rescale 操作，这样就可以减少除法运算。以及保留了部分数据用于
backpass。</p>
<p>从公式 15 复制一份过来，然后修改成公式 17：</p>
<p><span class="math display">\[
\begin{align}
    x_i&amp;=Q[k,:]K^T[:,i]\\
    m_i&amp;=\max \left( m_{i-1},x_i \right)\\
    l_{i}^{\prime}&amp;=l_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}\\
    \boldsymbol{o}_{i}^{\prime}&amp;=\boldsymbol{o}_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i}V[i,:]\\
\end{align} \tag{17}
\]</span></p>
<p><span class="math display">\[
O[k,:]=\frac{\boldsymbol{o}_N^{\prime}}{l_{N}^{\prime}}
\]</span></p>
<p><strong>调整了内外循环，Q 为外层循环，KV 为内层循环，减少 HBM
读写。</strong></p>
<p>在 FA1 中，先在外层循环 <span
class="math inline">\(K,V\)</span>，然后内层再循环 <span
class="math inline">\(Q\)</span>，但是内层循环每个 block 只会用到 <span
class="math inline">\(Q\)</span>
的一部分，这个在下面的示意图上可以明显地看出来。也就是 FA 1 只在
batch_size 和 head 上做并行，这样当 batch_size
比较小的时候，可能会导致启动的 block 太少，占不满 GPU 的 SM
计算资源。</p>
<p>FA2 调整了循环顺序， 外层是 <span class="math inline">\(Q\)</span>
循环，这样就可以在 seq len 上做并行，提高计算资源的使用率。</p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-10.png" class="" title="Alt text">
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-11.png" class="" title="Alt text">
<p><strong>调整 warp 划分策略。</strong></p>
<img data-src="/2024/05/30/FlashAttention%EF%BC%9A%E4%BB%8E-1-%E5%88%B0-3/image-12.png" class="" title="Alt text">
<p>FA1 中，warp1 的结果依赖于 warp234，这样 warp 之间就需要通信，但是
FA2 中每个 warp 间的计算是无关的。</p>
<p>FA2 的代码实现可以参考 [[FlashAttention CUTLASS 实现]]</p>
<h1 id="flashattention3">FlashAttention3</h1>
<p>FlashAttention 3 针对 Hopper 架构做了针对性的优化，包括
warp-specialized 的生产者-消费者流水线、将 softmax 计算与矩阵乘的异步
WGMMA 计算重叠，FP8 低精度加速。</p>

<div id="gitalk-container"></div>
<script src="https://cdn.bootcss.com/blueimp-md5/2.12.0/js/md5.min.js"></script><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

		<script>
		var gitalkConfig = {"clientID":"b72d5298f3697eb74696","clientSecret":"4cb6a985e89acb4525561d9c1cc12c589a7dccf6","repo":"Miroier.github.io","owner":"Miroier","admin":["Miroier"],"distractionFreeMode":false};
	    gitalkConfig.id = md5(location.pathname);
		var gitalk = new Gitalk(gitalkConfig);
	    gitalk.render("gitalk-container");
	    </script>
    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/02/02/Stream-K/" rel="prev" title="Stream-K">
                  <i class="fa fa-angle-left"></i> Stream-K
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2018 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Miroier</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">176k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:40</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Miroier" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>



  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



</body>
</html>
